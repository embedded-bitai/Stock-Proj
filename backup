import pandas as pd


# class StockCrow(form_class):
#     def __init__(self):
#         super().__init__()
self.code_df = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13',header=0)[0]

# 종목코드가 6자리이기 때문에 6자리를 맞춰주기 위해 설정해둠
code_df.종목코드 = code_df.종목코드.map('{:06d}'.format)

# 회사명과 종목코드 필요 -> 그 이외에 필요 없는 column 제외
code_df = code_df[['회사명', '종목코드']]

# 한글로된 컬럼명을 영어로 변환
code_df = code_df.rename(columns={'회사명' : 'name', '종목코드' : 'code'})
code_df.head() 
print(code_df.head())

# https://finance.naver.com/item/sise.nhn?code=005930(삼성전자)
def get_url(item_name, code_df):
    code = code_df.query("name=='{}'".format(item_name))['code'].to_string(index=False)
    code = code.strip()
    url = 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=code)



    print("요청 URL = {}".format(url))
    return url

# 삼성전자의 일자 데이터 url
item_name='삼성전자' 
url = get_url(item_name, code_df)


# 일자 데이터를 담을 df라는 DataFrame 정의
df = pd.DataFrame()

# 1페이지에서 15페이지의 데이터만 가져오기(약 6개월치)
for page in range(1, 16): 
    pg_url = '{url}&page={page}'.format(url=url, page=page) 
    df = df.append(pd.read_html(pg_url, header=0)[0], ignore_index=True)

# df.dropna()를 이용해 결측값 있는 행 제거
df = df.dropna()

# 한글 -> 영어
df = df.rename(columns= {
    '날짜': 'date', '종가': 'close', '전일비': 'diff', '시가': 'open',
    '고가': 'high', '저가': 'low', '거래량': 'volume'
    })

# 데이터 타입 int 변환
df[['close', 'diff', 'open', 'high', 'low', 'volume']] \
    = df[['close', 'diff', 'open', 'high', 'low', 'volume']].astype(int)

# date를 date type 변환
df['date'] = pd.to_datetime(df['date'])

# date 기준으로 내림차순 sort
df = df.sort_values(by=['date'], ascending=False)

# 상위 5개 데이터 확인
df.head()
print(df.head())
#


###

import stockCrow as sc
from flask import Flask

class MyStockApp():
    app = Flask(__name__)

    @app.route('/')

    def index():
        return 'Hello World!'

if __name__ == '__main__':
    myApp = MyStockApp()
    myApp.app.run(debug=True) 


###

import pandas as pd


class StockCrow(form_class):
    def __init__(self):
        self.get_url()

        ####
        self.code_df = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13',header=0)[0]

        # 종목코드가 6자리이기 때문에 6자리를 맞춰주기 위해 설정해둠
        self.code_df.종목코드 = code_df.종목코드.map('{:06d}'.format)

        # 회사명과 종목코드 필요 -> 그 이외에 필요 없는 column 제외
        self.code_df = code_df[['회사명', '종목코드']]

        # 한글로된 컬럼명을 영어로 변환
        self.code_df = code_df.rename(columns={'회사명' : 'name', '종목코드' : 'code'})
        self.code_df.head() 
        print(code_df.head())

        # 삼성전자의 일자 데이터 url
        self.item_name='삼성전자' 
        self.url = get_url(item_name, code_df)

        # 일자 데이터를 담을 df라는 DataFrame 정의
        self.df = pd.DataFrame()

        # 1페이지에서 15페이지의 데이터만 가져오기(약 6개월치)
        for page in range(1, 16): 
            self.pg_url = '{url}&page={page}'.format(url=url, page=page) 
            self.df = df.append(pd.read_html(pg_url, header=0)[0], ignore_index=True)

        # df.dropna()를 이용해 결측값 있는 행 제거
        self.df = df.dropna()

        # 한글 -> 영어
        df = df.rename(columns= {
            '날짜': 'date', '종가': 'close', '전일비': 'diff', '시가': 'open',
            '고가': 'high', '저가': 'low', '거래량': 'volume'
            })

        # 데이터 타입 int 변환
        df[['close', 'diff', 'open', 'high', 'low', 'volume']] \
            = df[['close', 'diff', 'open', 'high', 'low', 'volume']].astype(int)

        # date를 date type 변환
        df['date'] = pd.to_datetime(df['date'])

        # date 기준으로 내림차순 sort
        df = df.sort_values(by=['date'], ascending=False)

        # 상위 5개 데이터 확인
        self.df.head()
        print(df.head())
        #

    # https://finance.naver.com/item/sise.nhn?code=005930(삼성전자)
    def get_url(item_name, code_df):
        code = code_df.query("name=='{}'".format(item_name))['code'].to_string(index=False)
        code = code.strip()
        url = 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=code)

        print("요청 URL = {}".format(url))
        return url



### __main__
# @app.route('/')

# def get():
#     return 'Hello World!'

# if __name__ == '__main__':
#     app.run(debug=True) 

#########################################################################################################################################################
                                                                StockCrow
#########################################################################################################################################################
import csv
import pymysql
import pandas as pd
pymysql.install_as_MySQLdb()
from sqlalchemy import create_engine
from com_blacktensor.ext.db import db, openSession, engine


class StockCrow():
    def __init__(self):
        # 한국거래소(krx)
        self.code_df = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13',header=0)[0]

        # 종목코드가 6자리이기 때문에 6자리를 맞춰주기 위해 설정해둠
        code_df.종목코드 = code_df.종목코드.map('{:06d}'.format)

        # 회사명과 종목코드 필요 -> 그 이외에 필요 없는 column 제외
        code_df = code_df[['회사명', '종목코드']]

        # 한글로된 컬럼명을 영어로 변환
        code_df = code_df.rename(columns={'회사명' : 'name', '종목코드' : 'code'})
        code_df.head() 
        print(code_df.head())

    # https://finance.naver.com/item/sise.nhn?code=005930(삼성전자)
    def get_url(item_name, code_df):
        code = code_df.query("name=='{}'".format(item_name))['code'].to_string(index=False)
        code = code.strip()

        url = 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=code)

        print("요청 URL = {}".format(url))
        return url

    # 삼성전자의 일자 데이터 url
    item_name=['셀트리온', '삼성전자', '하나투어']

for i in item_name:
    print('\n'+i)
    url = get_url(i, code_df)
    # code 분리
    code_url = url.split('code=')
    code = 'a' + code_url[1]
    print(code)

    # 일자 데이터를 담을 df라는 DataFrame 정의
    df = pd.DataFrame()

    # 1페이지에서 15페이지의 데이터만 가져오기(약 6개월치)
    for page in range(1, 16): 
        pg_url = '{url}&page={page}'.format(url=url, page=page) 
        df = df.append(pd.read_html(pg_url, header=0)[0], ignore_index=True)

    # df.dropna()를 이용해 결측값 있는 행 제거
    df = df.dropna()

    # 한글 -> 영어
    df = df.rename(columns= {
        '날짜': 'date', '종가': 'close', '전일비': 'diff', '시가': 'open',
        '고가': 'high', '저가': 'low', '거래량': 'volume'
        })

    # 데이터 타입 int 변환
    df[['close', 'diff', 'open', 'high', 'low', 'volume']] \
        = df[['close', 'diff', 'open', 'high', 'low', 'volume']].astype(int)

    # date를 date type 변환
    df['date'] = pd.to_datetime(df['date'])

    # date 기준으로 내림차순 sort
    df = df.sort_values(by=['date'], ascending=False)
    
    df.head()
    print('-------------------- head -------------------')
    print(df.head())
    print('\n-------------------- 전체 -------------------')
    print(df)
    
    # csv file 저장
    df.to_csv(i + '.csv', mode = 'a', header = False)

    # Mysql Table이 존재하지 않다면 코드 이름으로 생성
    sql_table = 'SHOW TABLES LIKE \'' + code + '\''
    result = cursor.execute(sql_table)
    if result == 0:
        sql_crTable = 'CREATE TABLE ' + code + ' (Date date not null primary key, close int(11), diff int(11), open int(11), high int(11), low int(11), volume int(11));'
        cursor.execute(sql_crTable)

        # Table에 Data Insert(replace)dd
        df.to_sql(name=code, con=engine, if_exists='replace')
        conn.commit
conn.close()

class StockDto(db.Model):
    __tablename__ = code
    __table_args__={'mysql_collate' : 'utf8_general_ci'}

    date : str = db.Column(db.String(10), primary_key = True, index = True)
    close : int = db.Column(db.Int)
    diff : int = db.Column(db.Int)
    open : int = db.Column(db.Int)
    high : int = db.Column(db.Int)
    low : int = db.Column(db.Int)
    volume : int = db.Column(db.Int)

    def __init__(self, date, close, diff, open, high, low, volume):
        self.date = date
        self.close = close
        self.diff = diff
        self.open = open
        self.high = high
        self.low = low
        self.volume = volume
    
    def __repr__(self):
        return f'Stock(date={self.date}, close={self.close}, diff={self.diff}\
            , open={self.open}, high={self.high}, low={self.low}, volume={self.volume})'

    

# class StockDB():
#     def init(self):
#         #Mysql 연결
#         engine = create_engine("mysql+mysqldb://blackTensorStock:"+"456123"+"@localhost/blacktensorstock")
#         conn = pymysql.connect(host='localhost', user='blackTensorStock', password="456123", db='blacktensorstock')
#         # cursor 객체 -> Fetch 동작 관리
#         cursor = conn.cursor()

##################################################################################################################################################################
                                                                TestCrow
##################################################################################################################################################################
from com_blacktensor.ext.db import db
from com_blacktensor.ext.routes import Resource
import csv
import pandas as pd
from sqlalchemy import create_engine

# ============================================================
# ==================                     =====================
# ==================         KDD         =====================
# ==================                     =====================
# ============================================================
class StockKdd(object):
    # 한국거래소(krx)
    code_df = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13',header=0)[0]

    # 종목코드가 6자리이기 때문에 6자리를 맞춰주기 위해 설정해둠
    code_df.종목코드 = code_df.종목코드.map('{:06d}'.format)

    # 회사명과 종목코드 필요 -> 그 이외에 필요 없는 column 제외
    code_df = code_df[['회사명', '종목코드']]

    # 한글로된 컬럼명을 영어로 변환
    code_df = code_df.rename(columns={'회사명' : 'name', '종목코드' : 'code'})
    code_df.head() 
    print(code_df.head())

# https://finance.naver.com/item/sise.nhn?code=005930(삼성전자)
def get_url(item_name, code_df):
    code = code_df.query("name=='{}'".format(item_name))['code'].to_string(index=False)
    code = code.strip()

    url = 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=code)

    print("요청 URL = {}".format(url))
    return url

# 삼성전자의 일자 데이터 url
item_name=['셀트리온', '삼성전자', '하나투어']

for i in item_name:
    print('\n'+i)
    url = get_url(i, code_df)
    # code 분리
    code_url = url.split('code=')
    code = 'a' + code_url[1]
    print(code)

    # 일자 데이터를 담을 df라는 DataFrame 정의
    df = pd.DataFrame()

    # 1페이지에서 15페이지의 데이터만 가져오기(약 6개월치)
    for page in range(1, 16): 
        pg_url = '{url}&page={page}'.format(url=url, page=page) 
        df = df.append(pd.read_html(pg_url, header=0)[0], ignore_index=True)

    # df.dropna()를 이용해 결측값 있는 행 제거
    df = df.dropna()

    # 한글 -> 영어
    df = df.rename(columns= {
        '날짜': 'date', '종가': 'close', '전일비': 'diff', '시가': 'open',
        '고가': 'high', '저가': 'low', '거래량': 'volume'
        })

    # 데이터 타입 int 변환
    df[['close', 'diff', 'open', 'high', 'low', 'volume']] \
        = df[['close', 'diff', 'open', 'high', 'low', 'volume']].astype(int)

    # date를 date type 변환
    df['date'] = pd.to_datetime(df['date'])

    # date 기준으로 내림차순 sort
    df = df.sort_values(by=['date'], ascending=False)
    
    df.head()
    print('-------------------- head -------------------')
    print(df.head())
    print('\n-------------------- 전체 -------------------')
    print(df)
    
    # csv file 저장
    df.to_csv(i + '.csv', mode = 'a', header = False)

    # Mysql Table이 존재하지 않다면 코드 이름으로 생성
    sql_table = 'SHOW TABLES LIKE \'' + code + '\''
    result = cursor.execute(sql_table)
    if result == 0:
        sql_crTable = 'CREATE TABLE ' + code + ' (Date date not null primary key, close int(11), diff int(11), open int(11), high int(11), low int(11), volume int(11));'
        cursor.execute(sql_crTable)

        # Table에 Data Insert(replace)dd
        df.to_sql(name=code, con=engine, if_exists='replace')
        conn.commit
conn.close()

##################################################################################################################################################################
                                                                stock_crow
##################################################################################################################################################################

# ============================================================
# ==================                     =====================
# ==================         KDD         =====================
# ==================                     =====================
# ============================================================
class StockKdd(object):
    def __init__(self, code_df, code, url):
        # 삼성전자의 일자 데이터 url
        self.item_name=['셀트리온', '삼성전자', '하나투어']
        self.code_df = code_df
        self.url = url
        self.code = code

    def get_code(self, code_df):
        # 한국거래소(krx)
        self.code_df = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13',header=0)[0]
        
        # 종목코드가 6자리이기 때문에 6자리를 맞춰주기 위해 설정해둠
        code_df.종목코드 = self.code_df.종목코드.map('{:06d}'.format)

        # 회사명과 종목코드 필요 -> 그 이외에 필요 없는 column 제외
        code_df = self.code_df[['회사명', '종목코드']]

        # 한글로된 컬럼명을 영어로 변환
        code_df = self.code_df.rename(columns={'회사명' : 'name', '종목코드' : 'code'})
        code_df.head()
        print(code_df.head())

    # https://finance.naver.com/item/sise.nhn?code=005930(삼성전자)
    def get_url(self, item_name, code_df, code):
        self.code = code_df.query("name=='{}'".format(item_name))['code'].to_string(index=False).strip()
        # code = code.strip()

        url = 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=code)

        print("요청 URL = {}".format(url))
        
        # return url

    def get_data(self, get_url, item_name, code_df):
        for i in item_name:
            print('\n'+i)
            url = get_url(i, code_df)

        # 일자 데이터를 담을 df라는 DataFrame 정의
        self.df = pd.DataFrame()

        # 1페이지에서 15페이지의 데이터만 가져오기(약 6개월치)
        for page in range(1, 16): 
            pg_url = '{url}&page={page}'.format(url=url, page=page) 
            df = df.append(pd.read_html(pg_url, header=0)[0], ignore_index=True)

        # df.dropna()를 이용해 결측값 있는 행 제거
        df = df.dropna()

        # df.drop()을 이용해 columns 삭제
        df = df.drop(columns= {'전일비', '고가', '저가'})

        # 한글 -> 영어
        df = df.rename(columns= {
            '날짜': 'date', '종가': 'close', '시가': 'open', '거래량': 'volume'})

        # 데이터 타입 int 변환
        df[['close', 'open', 'volume']] = df[['close', 'open', 'volume']].astype(int)

        # date를 date type 변환
        df['date'] = pd.to_datetime(df['date'])

        # date 기준으로 내림차순 sort
        df = df.sort_values(by=['date'], ascending=False)
        
        df.head()
        print('-------------------- head -------------------')
        print(df.head())
        print('\n-------------------- 전체 -------------------')
        print(df)
        
        # csv file 저장
        df.to_csv(i + '.csv', mode = 'a', header = False)


        # Mysql Table이 존재하지 않다면 코드 이름으로 생성
        sql_table = 'SHOW TABLES LIKE \'' + code + '\''
        result = cursor.execute(sql_table)
        if result == 0:
            sql_crTable = 'CREATE TABLE ' + code + ' (Date date not null primary key, close int(11), diff int(11), open int(11), high int(11), low int(11), volume int(11));'
            cursor.execute(sql_crTable)

            # Table에 Data Insert(replace)dd
            df.to_sql(name=code, con=engine, if_exists='replace')
            conn.commit
    conn.close()

===================================================================================================================================================================================
                                crow_naver
===================================================================================================================================================================================
##  HeadLine
# pip install lxml
import requests
import pandas as pd
from bs4 import BeautifulSoup
class CrowKdd(object):
    # results = ['']
    keyword = '삼성전자'
    def naver_news(self, keyword, order):
        # tag = ['']
        results = []
        # a = ''
        
        for i in range(20)[1:]:
            url = r'https://search.naver.com/search.naver?&where=news&query={}&sm=tab_pge&sort={}&&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&cluster_rank=287&start={}&refresh_start=0'.format(keyword, order, 10*(i-1)+1)
            resp = requests.get(url)
            soup = BeautifulSoup(resp.text, 'lxml')
        #     article_title = soup.find_all('a', class_ = 'news_tit')
            

        #     for j in article_title:
        #         a += j.get_text()
        # return a


            title_list = soup.find_all('a', class_ = 'news_tit')
            

            for tag in title_list:
                # results += tag.get_text()
                results.append(tag.text)
        return results

            # # title_list = soup.select('.news_tit')
            # title_list = soup.find_all('a', class_ = 'news_tit')

            # for tag in title_list:
            # #     # print(tag.text)
            # #     # df = pd.DataFrame(tag.text)
            # #     # print(df.head())
            #     # results.append(tag.text)
            #     results += tag.get_text()
            # return results

    result = naver_news(object, keyword, 1) # 0 = 관련도순, 1 = 최신순, 2 = 오래된 순
    # print(result)
    df = pd.DataFrame(result)
    # print(df)
    df.columns = ['title']
    print(df.head())
    df.to_csv(keyword + '.csv', encoding='utf8')



=========================================================================================================================================================
                                            긍정 부정 카운팅 crow_emotion 백업본
=========================================================================================================================================================

import requests
import pandas as pd
import codecs
from bs4 import BeautifulSoup
from konlpy.tag import Twitter
from collections import Counter
from com_blacktensor.ext.db import db, openSession, engine

# import time
# import multiprocessing


# ============================================================
# ==================                     =====================
# ==================         KDD         =====================
# ==================                     =====================
# ============================================================
# class CrowKdd(object):
# def get_url():
# search_keyword = '애플'
# url = f'https://search.naver.com/search.naver?where=news&sm=tab_jum&query={search_keyword}'

# r = requests.get(url)
# req = requests.get('https://search.naver.com/search.naver?where=news&sm=tab_jum&query=셀트리온')
# html = req.text
# soup = BeautifulSoup(html, 'html.parser')
# news_titles = soup.select('.news .type01 li dt a[title]')

# print('new: ', len(news_titles))
# print()
# for title in news_titles:
#     print(title['title'])

# def get_news(n_url):
#     news_detail = []
#     print(n_url)
#     breq = requests.get(n_url)
#     bsoup = BeautifulSoup(breq.content, 'html.parser')

#     title = bsoup.select('h3#articleTitle')[0].text
#     news_detail.append(title)

#     pdate = bsoup.select('.t11')[0].get_text()[:11]
#     news_detail.append(pdate)

#     _text = bsoup.select('#articleBodyContents')[0].get_text().replace('\n', " ")
#     btext = _text.replace("// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}", "")

#     news_detail.append(btext.strip())

#     pcompany = bsoup.select('#footer address')[0].a.get_text()
#     news_detail.append(pcompany)

#     return news_detail

#     print(news_detail)

# query = "삼성전자"
# s_date = "2020.04.01"
# e_date = "2018.10.30"
# s_from = s_date.replace(".","")
# e_to = e_date.replace(".","")
# page = 1

# f = open("C:/Users/Admin/VscProject/BlackTensor_Test/" + query + '.csv', 'w', encoding='utf-8')

# while page < 100:

#     print(page)

#     url = "https://search.naver.com/search.naver?where=news&query=" + query + "&sort=1&ds=" + s_date + "&de=" + e_date + "&nso=so%3Ar%2Cp%3Afrom" + s_from + "to" + e_to + "%2Ca%3A&start=" + str(page)

#     header = {
#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
#     }
#     req = requests.get(url,headers=header)
#     print(url)
#     cont = req.content
#     soup = BeautifulSoup(cont, 'html.parser')
    
#     for urls in soup.select("._sp_each_url"):
#         try :
#             if urls["href"].startswith("https://news.naver.com"):
#                 news_detail = get_news(urls["href"])
#                 f.write("{}\t{}\t{}\t{}\n".format(news_detail[1], news_detail[3], news_detail[0],
#                                                       news_detail[2]))
#         except Exception as e:
#             # print(e) 
#             continue
#     page += 10  
# f.close()

# url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query=코알라'

# r = requests.get(url)
# html = r.content
# soup = BeautifulSoup(html, 'html.parser')
# titles_html = soup.select('.new_area > a.news_tit')

# for i in range(len(titles_html)):
#     print(i+1, titles_html[i].text)

##################################################################################################################################################

# ### HeadLine
class CrawKdd(object):
    # ##keyword = '삼성전자'
    info_main = input("="*50+"\n"+"입력 형식에 맞게 입력해주세요."+"\n"+"시작하시려면 Enter를 눌러주세요."+"\n"+"="*50)
    maxpage = int(input("최대 크롤링할 페이지 수 입력하시오: "))
    keyword = input("검색어 입력: ")
    order = input("뉴스 검색 방식 입력(관련도순=0 최신순=1 오래된순=2): ") #관련도순=0 최신순=1 오래된순=2
    s_date = input("시작날짜 입력(예: 2020.07.20):")
    e_date = input("끝날짜 입력(예: 2020.10.30):")
    date_text = []
    def __init__(self):
        info_main = self.info_main
        maxpage = self.maxpage
        keyword = self.keyword
        order = self.order
        s_date = self.s_date
        e_date = self.e_date
        date_text = self.date_text

    def naver_news(self, maxpage, keyword, order, s_date, e_date):
        # tag = ['']
        # date_text = ''
        results = []
        data_results = []
        date_text = []
        test_date = []
        # a = ''
        # https://search.naver.com/search.naver?where=news&query={}&sm=tab_opt&sort={}&photo=0&field=0&reporter_article=&pd=3&ds={}&de={}&docid=&nso=so%3Ar%2Cp%3Afrom20201020to20201030%2Ca%3Aall&mynews=0&refresh_start={}&related=0
        # url = r'https://search.naver.com/search.naver?&where=news&query={}&sm=tab_pge&sort={}&photo=0&field=0&reporter_article=&pd=3&ds={}&de={}&docid=&nso=so:da,p:from20201028to20201030,a:all&mynews=0&start={}&refresh_start=0'.format(keyword, order, s_date, e_date, 10*(i-1)+1)
        # url = "https://search.naver.com/search.naver?where=news&query=" + query + "&sort=1&ds=" + s_date + "&de=" + e_date + "&nso=so%3Ar%2Cp%3Afrom" + s_from + "to" + e_to + "%2Ca%3A&start=" + str(page)
        for i in range(maxpage)[1:]:
            url = r'https://search.naver.com/search.naver?&where=news&query={}&sm=tab_pge&sort={}&photo=0&field=0&reporter_article=&pd=3&ds={}&de={}&docid=&nso=so:da,p:from20201028to20201030,a:all&mynews=0&start={}&refresh_start=0'.format(keyword, order, s_date, e_date, 10*(i-1)+1)
            # url = r'https://search.naver.com/search.naver?&where=news&query={}&sm=tab_pge&sort={}&&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&cluster_rank=287&start={}&refresh_start=0'.format(keyword, order, 10*(i-1)+1)
            resp = requests.get(url)
            soup = BeautifulSoup(resp.text, 'lxml')
            if i % 100 == 0:
                print(i,"번째 크롤링")
        #     article_title = soup.find_all('a', class_ = 'news_tit')
            

        #     for j in article_title:
        #         a += j.get_text()
        # return a

            # # title_list = soup.select('.news_tit')
            # title_list = soup.find_all('a', class_ = 'news_tit')

            # for tag in title_list:
            # #     # print(tag.text)
            # #     # df = pd.DataFrame(tag.text)
            # #     # print(df.head())
            #     # results.append(tag.text)
            #     results += tag.get_text()
        # return results

            title_list = soup.find_all('a', class_ = 'news_tit')

            for tag in title_list:
                # results += tag.get_text()
                results.append(tag.text)

### ------------------------------------------------------
            date_lists = soup.select('span.info')

            for date_list in date_lists:
                test = date_list.text

            # print("날짜 데이터!!!!: ", test)
            try:
                pattern = '\d+.(\d+).(\d+).'
                r = re.compile(pattern)
                match = r.search(test)#.group(0) # 2018.11.05.
                date_text.append(match)
                test_date.append(test)
            except AttributeError:

                pattern = '\w* (\d\w*)'

                r = re.compile(pattern)
                match = r.search(test)#.group(1)
                #print(match)
                date_text.append(match)
                test_date.append(test)
            
        # print("크롤링 날짜!! :", date_text)
        # print("날짜 데이터!!!!: ", test_date)

### ------------------------------------------------------
        return results
            

            # #날짜 추출
            # date_lists = soup.select('.info_group')
            # for date_list in date_lists:
            #     test=date_list.text
            #     DateCleansing(0, test)

        #     for tag in title_list:
        #         # results += tag.get_text()
        #         data_results.append(tag.text)
        #     new_date = {"date" : date_text}
        # return data_results

    # def DateCleansing(self, test):
    #     try:
    #         pattern = '\d+.(\d+).(\d+).'
    #         r = re.compile(pattern)
    #         match = r.search(test).group(0) # 2018.11.05.
    #         date_text.append(match)
    #     except AttributeError:

    #         pattern = '\w* (\d\w*)'

    #         r = re.compile(pattern)
    #         match = r.search(test).group(1)
    #         #print(match)
    #         date_text.append(match)
    
    # print("크롤링 날짜!! :",date_text)

    # result = naver_news(object, keyword, 1)
    result = naver_news(object, maxpage, keyword, order, s_date, e_date)
    # print(result)
    df = pd.DataFrame(result)
    # print(df)
    df.columns = ['title']
    print(df.head())
    df.to_csv(keyword + '.csv', encoding='utf8')
'''
0   논어, 새로운 가르침에 겁내지 않으려면 그간의 가르침을 실행해야 한다!       
1  "전 세계 AI 전문가 모여라"…'삼성 AI 포럼 2020' 온라인 개최
2              비트코인 지갑서비스 사업자도 자금세탁방지 의무 부과
3                  [연합뉴스 이 시각 헤드라인] - 12:00
4   “이건희 회장의 ‘도전 DNA’ 계승… 판도 바꾸는 기업으로 진화하자”
'''


# # =======================================================================================================================================

# # # title_text = []
# # # link_text = []
# # # source_text = []
# # # date_text = []
# # # contents_text = []

# # def main():
# #     info_main = input("="*50+"\n"+"입력 형식에 맞게 입력해주세요."+"\n"+" 시작하시려면 Enter를 눌러주세요."+"\n"+"="*50)
# #     maxpage = input("최대 크롤링할 페이지 수 입력하시오: ")
# #     query = input("검색어 입력: ")
# #     sort = input("뉴스 검색 방식 입력(관련도순=0 최신순=1 오래된순=2): ") #관련도순=0 최신순=1 오래된순=2
# #     s_date = input("시작날짜 입력(2019.01.04):") #2019.01.04
# #     e_date = input("끝날짜 입력(2019.01.05):") #2019.01.05
# #     crawler(maxpage, query, sort, s_date, e_date)
# # main()

# # def crawler(maxpage, query, sort, s_date, e_date):
# #     s_from = s_date.replace(".","")
# #     e_to = e_date.replace(".","")
# #     page = 1
# #     maxpage_t =(int(maxpage)-1)*10+1 # 11= 2페이지 21=3페이지 31=4페이지 ...81=9페이지 , 91=10페이지, 101=11페이지

# #     while page <= maxpage_t:
# #         url = "https://search.naver.com/search.naver?where=news&query=" + query + "&sort="+sort+"&ds=" + s_date + "&de=" + e_date + "&nso=so%3Ar%2Cp%3Afrom" + s_from + "to" + e_to + "%2Ca%3A&start=" + str(page)
# #         response = requests.get(url)
# #         html = response.text

# #         #뷰티풀소프의 인자값 지정
# #         soup = BeautifulSoup(html, 'html.parser')

# #         #태그에서 제목과 링크주소 추출
# #         atags = soup.select('.news_tit')
# #         for atag in atags:
# #             title_text.append(atag.text)
# #             link_text.append(atag['href'])

# #         #신문사 추출
# #         source_lists = soup.select('.thumb_box')
# #         for source_list in source_lists:
# #             source_text.append(source_list.text)

# #         #날짜 추출
# #         date_lists = soup.select('.info')
# #         for date_list in date_lists:
# #             test=date_list.text
# #             date_cleansing(test)

# #         #본문요약본
# #         contents_lists = soup.select('a.api_txt_lines.dsc_txt_wrap')
# #         for contents_list in contents_lists:
# #             #print('==='*40)
# #             #print(contents_list)
# #             contents_cleansing(contents_list)

# #         #모든 리스트 딕셔너리형태로 저장
# #         result= {"date" : date_text , "title":title_text , "source" : source_text ,"contents": contents_text ,"link":link_text }
# #         print(page)

# #         df = pd.DataFrame(result) #df로 변환
# #         page += 10

# # #날짜 정제화 함수
# # def date_cleansing(test):
# #     try:
# #         pattern = '\d+.(\d+).(\d+).'
# #         r = re.compile(pattern)
# #         match = r.search(test).group(0) # 2018.11.05.
# #         date_text.append(match)
# #     except AttributeError:

# #         pattern = '\w* (\d\w*)'

# #         r = re.compile(pattern)
# #         match = r.search(test).group(1)
# #         #print(match)
# #         date_text.append(match)

# # #내용 정제화 함수
# # def contents_cleansing(contents):
# #     first_cleansing_contents = re.sub('<dl>.*?</a> </div> </dd> <dd>', '',
# #     str(contents)).strip()
# #     second_cleansing_contents = re.sub('<ul class="relation_lst">.*?</dd>', '',
# #     first_cleansing_contents).strip()
# #     third_cleansing_contents = re.sub('<.+?>', '', second_cleansing_contents).strip()
# #     contents_text.append(third_cleansing_contents)
# #     #print(contents_text)


# # #엑셀로 저장하기 위한 변수
# # RESULT_PATH ='C:/Users/User/Desktop/python study/beautifulSoup_ws/crawling_result/'
# # now = datetime.now()

# # # 새로 만들 파일이름 지정
# # outputFileName = '%s-%s-%s %s시 %s분 %s초 merging.xlsx' % (now.year, now.month, now.day, now.hour, now.minute, now.second)
# # df.to_excel(RESULT_PATH+outputFileName,sheet_name='sheet1')
# # =======================================================================================================================================

# # from bs4 import BeautifulSoup
# # import requests

# # # url = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query=삼성전자'
# # url = "https://search.naver.com/search.naver?where=news&query=" + query + "&sort="+sort+"&ds=" + s_date + "&de=" + e_date + "&nso=so%3Ar%2Cp%3Afrom" + s_from + "to" + e_to + "%2Ca%3A&start=" + str(page)
# # # https://search.naver.com/search.naver?where=news&query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&sm=tab_srt&sort=1&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so%3Add%2Cp%3Aall%2Ca%3Aall&mynews=0&refresh_start=0&related=0
# # res = requests.get(url)
# # html = res.text

# # soup = BeautifulSoup(html, 'html.parser')

# # title_list = soup.select('.news_tit')

# # for tag in title_list:
# #     print(tag.text)

#     # ### 네이버 금융 재무정보 크롤링 테스트
#     # def NaverCrawStock(self):
#     #     url_tmp = 'https://finance.naver.com/item/main.nhn?code%s'
#     #     url = url_tmp % ('005930')

#     #     item_info = requests.get(url).text
#     #     soup = BeautifulSoup(item_info, 'html.parser')
#     #     finance_info = soup.select('div.section.cop_analysis div.sub_section')[0]

#     #     th_data = [item.get_text().strip() for item in finance_info.select('thead th')]
#     #     annual_date = th_data[3:7]
#     #     quarter_date = th_data[7:13]

#     #     finance_index = [item.get_text().strip() for item in finance_info.select('th.h_th2')][3:0]

#     #     finance_data = [item.get_text().stipt() for item in finance_info.select('td')]
#     #     finance_data = np.array(finance_data)
#     #     finance_data.resize(len(finance_index), 10)

#     #     finance_date = annual_date + quarter_date

#     #     finance = pd.DataFrame(data=finance_data[0:,0:], index=finance_index, columns = finance_date)


#     # ### Nice 평가정보 재무정보 크롤링 테스트
#     # def NiceCraw(self):
#     #     url_tmp = 'http://media.kisline.com/highlight/mainHighlight.nice?nav=1&paper_stock=%s'
#     #     url = url_tmp % ('005930')
#     #     tables = pd.read_html(url)
#     #     df = tables[4]

# # ### Naver 금융 재무정보 2 크롤링 테스트
# #     def NaverCrawTmp(self):
# #         url_tmp = 'https://finance.naver.com/item/main.nhn?code%s'
# #         url = url_tmp % ('005930')
# #         tables = pd.read_html(url, encoding='euc-kr')
# #         df = tables[3]

# # =======================================================================================================================================

# # ============================================================
# # ==================                     =====================
# # ==================    Preprocessing    =====================
# # ==================                     =====================
# # ============================================================
class CrawDf(object):
    def __init__(self):
        self.ck = CrawKdd()
        this = self.ck
        self.keyword = this.keyword
        print("검색어1: ", self.keyword)

        # this.maxpage = self.maxpage
        # this.keyword = self.ck.keyword
        # this.order = self.order
        # this.s_date = self.s_date
        # this.e_date = self.e_date

        self.word = []
        self.noun_list =[]
        self.positive_word = []
        self.negative_word = []

        self.poflag = []
        self.neflag = []

        self.po_key = []
        self.ne_key = []
        self.po_val = []
        self.ne_val = []

        self.stock_name = []

    # def DataPro(self, keyword, word, positive_word, negative_word, poflag, neflag):
    def DataPro(self):
        # 
        keyword = str(self.keyword)
        word = self.word
        noun_list = self.noun_list
        positive_word = self.positive_word
        negative_word = self.negative_word
        poflag = self.poflag
        neflag = self.neflag
        po_key = self.po_key
        ne_key = self.ne_key
        po_val = self.po_val
        ne_val = self.ne_val
        stock_name = self.stock_name
        # this = self.ck
        # this.keyword = self.keyword

        # print("검색어2: ", keyword)
        # 
        # word = []
        # noun_list =[]
        # positive_word = []
        # negative_word = []
        # keyword_text = []

        # poflag = []
        # neflag = []

        

        file = open('{}.csv'.format(keyword), 'r', encoding='utf-8')

        # file = open('삼성전자.csv', 'r', encoding='utf-8')
        lists = file.readlines()
        file.close()
        # lists
        
        twitter = Twitter()
        morphs = []

        for sentence in lists:
            morphs.append(twitter.pos(sentence))

        # print(morphs)

        pos = codecs.open('positive_words_self.txt', 'rb', encoding='UTF-8')

        while True:
            line = pos.readline()
            line = line.replace('\n', '')
            positive_word.append(line)
            # keyword_text.append(line)

            if not line: break
        pos.close()

        neg = codecs.open('negative_words_self.txt', 'rb', encoding='UTF-8')

        while True:
            line = neg.readline()
            line = line.replace('\n', '')
            negative_word.append(line)
            # keyword_text.append(line)

            if not line: break
        neg.close()

    # #     # for sentence in morphs : 
    # #     #     for word, text_tag in sentence :
    # #     #         for x in range(len(keyword_text)):
    # #     #             posflag = False
    # #     #             negflag = False

    # #     #             if x < len(positive_word)-1:
    # #     #                 if word.find(keyword_text[x] != -1):
    # #     #                     posflag = True
    # #     #                     print(x, "positive_word?", "테스트 : ", word.find(keyword_text[x]), "비교 단어 : ", keyword_text[x], "인덱스 : ", x, word)
    # #     #                     break
    # #     #             if x > (len(positive_word)-2):
    # #     #                 if word.find(keyword_text[x] != -1):
    # #     #                     negflag = True
    # #     #                     print(x, "negative?","테스트 : ", word.find(keyword_text[i]),"비교단어 : ", keyword_text[i], "인덱스 : ", x, word)
    # #     #                     break

    # #     #                 if posflag == True:
        

    # #     # print(type(positive_word))




    # # #==========================================================================================================
        for sentence in morphs : 
            for word, text_tag in sentence :
                if text_tag in ['Noun']:
                    noun_list.append(word)
                    for x in positive_word:
                        if x == word: 
                            poflag.append(x)
                        
                    for y in negative_word:
                        if y == word:
                            neflag.append(y)

                #         print("부정적 :", y)
                # if text_tag in ['Noun'] and ("것" not in word) and ("내" not in word) and ("첫" not in word) and \
                #     ("나" not in word) and ("와" not in word) and ("식" not in word) and ("수" not in word) and \
                #     ("게" not in word) and ("말" not in word):
                #      noun_list.append(word)
                    
                # if text_tag in ['Noun'] and ("갑질" not in word) and ("논란" not in word) and ("폭리" not in word) and \
                #     ("허위" not in word) and ("과징금" not in word) and ("눈물" not in word) and ("피해" not in word) and \
                #     ("포화" not in word) and ("우롱" not in word) and ("위반" not in word) and ("리스크" not in word) and \
                #     ("사퇴" not in word) and ("급락" not in word) and ("하락" not in word) and ("폐업" not in word) and \
                #     ("불만" not in word) and ("산재" not in word) and ("닫아" not in word) and ("손해배상" not in word) and \
                #     ("구설수" not in word) and ("적발" not in word) and ("침해" not in word) and ("빨간불" not in word) and \
                #     ("취약" not in word) and ("불명예" not in word) and ("구형" not in word) and ("기소" not in word) and \
                #     ("반토막" not in word) and ("호소" not in word) and ("불매" not in word) and ("냉담" not in word) and \
                #     ("문제" not in word) and ("직격탄" not in word) and ("한숨" not in word) and ("불똥" not in word) and \
                #     ("항의" not in word) and ("싸늘" not in word) and ("일탈" not in word) and ("파문" not in word) and \
                #     ("횡령" not in word) and ("사과문" not in word) and ("여파" not in word) and ("울상" not in word) and \
                #     ("초토화" not in word) and ("급감" not in word) and ("우려" not in word) and ("중단" not in word) and \
                #     ("퇴출" not in word) and ("해지" not in word) and ("일베" not in word) and ("이물질" not in word) and \
                #     ("엉망" not in word) and ("소송" not in word) and ("하락" not in word) and ("매출하락" not in word) and \
                #     ("혐의" not in word) and ("부채" not in word) and ("과징금" not in word) and ("포기" not in word) and \
                #     ("약세" not in word) and ("최악" not in word) and ("손실" not in word) and ("의혹" not in word):
                #     positive_word.append(word)

                # elif text_tag in ['Noun'] and ("MOU" not in word) and ("제휴" not in word) and ("주목" not in word) and \
                #     ("호응" not in word) and ("돌파" not in word) and ("이목" not in word) and ("수상" not in word) and \
                #     ("입점" not in word) and ("인기" not in word) and ("열풍" not in word) and ("진화" not in word) and \
                #     ("대박" not in word) and ("순항" not in word) and ("유치" not in word) and ("1위" not in word) and \
                #     ("출시" not in word) and ("오픈" not in word) and ("돌풍" not in word) and ("인싸" not in word) and \
                #     ("줄서서" not in word) and ("대세" not in word) and ("트렌드" not in word) and ("불티" not in word) and \
                #     ("진출" not in word) and ("체결" not in word) and ("증가" not in word) and ("기부" not in word) and \
                #     ("신제품" not in word) and ("신상" not in word) and ("최고" not in word) and ("새로운" not in word) and \
                #     ("착한" not in word) and ("신기록" not in word) and ("전망" not in word) and ("협력" not in word) and \
                #     ("역대" not in word) and ("상승" not in word) and ("늘어" not in word) and ("승인" not in word):
                #     negative_word.append(word)

    # #     # print(noun_list)
        
    # #     # count = Counter(noun_list)
    # #     # words = dict(count.most_common())
    # #     # print(words)
        
    # #     # print(positive_word)
    # #     # print(negative_word)
        count_po = Counter(poflag)
        count_ne = Counter(neflag)
    #     # po_words = count_po.most_common()
        po_words = dict(count_po.most_common())
        ne_words = dict(count_ne.most_common())

        # 워드클라우드로 명사만 추출
        # print(noun_list)
        '''
        ['창립', '주년', '삼성', '전자', '이건희', '회장', '도전', '혁신', '삼성', '전자', '삼성', '포럼', '개최', '김기남', '대표', 
        '핵심', '기술', '발전', '현', '코스피', '코스닥', '장', '동반', '상승', '덕성', '시스', '웍', '한국', '컴퓨터', '삼성', '전자
        ', '창립', '주년', '기념', '개최', '이재용', '부회장', '불참', '롯데', '하이마트', '온라인', '오늘', '역대', '빅', '하트', ' 
        일', '시작', '손연기', '칼럼', '차', '산업혁명', '시대', '문제', '일자리', '삼성', '전자', '모바일', '신제품', '엑시노스', ' 
        ...
        '멘토', '체험', '활동', '김기남', '삼성', '부회장', '로', '코로나', '해결', '위해', '전세계', '연구자', '협력', '순위', '주식
        ', '부자', '위', '눈앞', '이재용', '뉴', '파워', '프라', '마', '규모', '유상증자', '결정', '삼성', '전자', '창립', '주념', ' 
        기념', '회장', '도전', '혁신', '계승', '삼성', '전자', '창립', '주년', '기념', '개최']
        '''

        po_key = po_words.keys()
        po_val = po_words.values()

        ne_key = ne_words.keys()
        ne_val = ne_words.values()
        # for key, value in po_words:
        #     po_key.append(key)
            
        #     # po_val.append(value)
        # print(po_key)
        # # print(po_val)

        # 
        print("\n긍정적인 단어 :", po_key, po_val)
        # print("긍정적인 단어", positive_word)
        # print(type(po_words))
        print("부정적인 단어 :", ne_key, ne_val)
        
        # word_df = {}

        po_df = pd.DataFrame(list(po_words.items()), columns=['positive', 'pos_count'])
        ne_df = pd.DataFrame(list(ne_words.items()), columns=['negative', 'neg_count'])

        df = pd.concat([po_df,ne_df], axis=1)

        df.loc[:, 'stock'] = keyword

        # df = pd.concat([df, stock_df], axis=1)


        # word_df = {'stock' : keyword,
        #            'positive' : po_key,
        #            'pos_count' : po_val,
        #            'negative' : ne_key,
        #            'neg_count' : ne_val}

        # df = pd.DataFrame.from_dict(word_df, orient='index')

        # word_df = {keyword, po_key, po_val, ne_key, ne_val}

        # df = pd.DataFrame.from_dict(word_df, orient='index').rename(
        #     columns={0:'index', 1:'positive', 2:'pos_count', 3:'negative', 4:'neg_count'})

        # df.transpose()

        # df.columns = ['index', 'positive', 'negative']
        print(df.head())
        df.to_csv(keyword + '_word.csv', encoding='utf8')

        '''
        긍정적인 단어 : {'상승': 141, '인기': 66, '출시': 60, '전망': 36, '오픈': 30, 
        '돌파': 19, '트렌드': 12, '체결': 12, '증가': 12, '역대': 11, '협력': 11, 
        '주목': 11, '미소': 8, '기부': 8, '승인': 6, '최고': 6, '대세': 5, '유치': 4, 
        '수상': 4, '불티': 2, '부상': 2, '순항': 2, '호응': 1, '진출': 1}
        부정적인 단어 : {'급감': 233, '여파': 163, '하락': 162, '피해': 115, 
        '직격탄': 83, '논란': 61, '중단': 41, '손실': 39, '반토 막': 34, '최악': 33, 
        '포기': 32, '폐업': 25, '급락': 25, '우려': 24, '불매': 14, '눈물': 13, '
        매각': 10, '호소': 9, '울상': 7, '문제': 6, '불만': 6, '약세': 5, '한숨': 5, 
        '일베': 4, '해지': 4, '초토화': 3, '참혹': 3, '폐점': 2, '파문': 2, 
        '과징금': 2, '항의': 1, '소송': 1, '불명예': 1, '리스크': 1, '갑질': 1, 
        '침해': 1, '발끈': 1}
        '''
# ck = CrawKdd(maxpage, keyword, order, s_date, e_date)

# ck = CrawKdd('maxpage', 'keyword', 'order', 's_date', 'e_date').format(ck.keyword)
# ck = CrawKdd.__init__(keyword)
# print(ck.keyword)



# ============================================================
# ==================                     =====================
# ==================       Modeling      =====================
# ==================                     =====================
# ============================================================
class CrawDto(db.Model):
    __tablename__ = 'new_emotion'
    __table_args__={'mysql_collate' : 'utf8_general_ci'}

    # date : str = db.Column(db.String(10), primary_key = True, index = True)
    # stock_name : str = db.Column(db.String(10))
    # positive : str = db.Column(db.String(10))
    # negative : str = db.Column(db.String(10))
    no = Column(Integer, primary_key=True)
    date = Column(Date)
    stock_name = Column(String)
    positive = Column(String)
    negative = Column(String)

    def __init__(self, no, date, stock_name, positive, negative):
        self.no = no
        self.date = date
        self.stock_name = stock_name
        self.positive = positive
        self.negative = negative
    
    def __repr__(self):
        timer = self.time.strftime('%Y-%m-%d')
        return f'new_emotion(no={self.no}, date={self.timer}, stock_name={self.stock_name}, \
         positive={self.positive}, negative={self.negative})'

    # @property
    # def json(self):
    #     return {
    #         'no': self.no,
    #         'date': self.time.strftime('%Y-%m-%d'),
    #         'stock_name': self.stock_name,
    #         'positive': self.positive,
    #         'negative': self.negative
    #     }

class CrawVo:
    no = int = 0
    date : str = ''
    stock_name : str = ''
    positive : str = ''
    negative : str = ''


Session = openSession()
session = Session()
craw_df = CrawDf()


class CrawDao(CrawDto):
    
    @staticmethod
    def bulk():
        Session = openSession()
        session = Session()
        craw_df = CrawDf()
        df = craw_df.hook()
        # print(df.head())
        session.bulk_insert_mappings(CrawDto, df.to_dict(orient='records'))
        session.commit()
        session.close()

    @staticmethod
    def count():
        return session.query(func.count(CrawDto.date)).one()

    @staticmethod
    def save(craw):
        new_craw = CrawDto(date = craw['date'],
                           stock_name = craw['stock_name'],
                           positive = craw['positive'],
                           negative = craw['negative'])
        session.add(new_craw)
        session.commit()
    print('Ok!')


# class CrawTf(object):
#     ...
# class CrawAi(object):
#     ...


if __name__ == "__main__":
    ck = CrawKdd()
    cd = CrawDf()
    cd.DataPro()

# ============================================================
# ==================                     =====================
# ==================      Resourcing     =====================
# ==================                     =====================
# ============================================================

# parser = reqparse.RequestParser()

# parser.add_argument('stock_name', type = str, required = True,
#                             help='This field should be a userId')
# parser.add_argument('positive', type = str, required = True,
#                             help='This field should be a password')
# parser.add_argument('negative', type = str, required = True,
#                             help='This field should be a password')

# class Craw(Resource):
    
#     @staticmethod
#     def post():
#         args = parser.parse_args()
#         craw = CrawVo()
#         craw.stock_name = args.stock_name
#         craw.positive = args.positive
#         craw.negative = args.negative
#         # service.assign(craw)
#         # print("Predicted Craw")

# =========================================================================================================================

# import requests # 웹 페이지 소스를 얻기 위한 패키지(기본 내장 패키지이다.)
# from bs4 import BeautifulSoup # 웹 페이지 소스를 얻기 위한 패키지, 더 간단히 얻을 수 있다는 장점이 있다고 한다.
# from datetime import datetime                                # (!pip install beautifulsoup4 으로 다운받을 수 있다.)
# import pandas as pd # 데이터를 처리하기 위한 가장 기본적인 패키지
# import time # 사이트를 불러올 때, 작업 지연시간을 지정해주기 위한 패키지이다. (사이트가 늦게 켜지면 에러가 발생하기 때문)
# import urllib.request #
# from selenium.webdriver import Chrome
# import json
# import re     
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.keys import Keys
# import datetime as dt

# def ttt(xx):
#     name = xx
#     base_url = 'https://finance.naver.com/item/coinfo.nhn?code='+ name + '&target=finsum_more'
#     return base_url
    
# browser  = Chrome()
# browser.maximize_window()

# browser.get(ttt('066571'))

# browser.switch_to_frame(browser.find_element_by_id('coinfo_cp')) #frame구조 안으로 들어가기

# #재무제표 "연간" 클릭하기
# browser.find_elements_by_xpath('//*[@class="schtab"][1]/tbody/tr/td[3]')[0].click()

# html0 = browser.page_source #지금 현 상태의 page source불러오기
# html1 = BeautifulSoup(html0,'html.parser')

# #기업 title불러오기
# title0 = html1.find('head').find('title').text
# title0.split('-')[-1]

# html22 = html1.find('table',{'class':'gHead01 all-width','summary':'주요재무정보를 제공합니다.'})
# #재무제표 영역 불러오기

# thead0 = html22.find('thead') #날짜가 재무제표영역의 head부분에 들어가 있기 때문에 thead를 불러와야 한다.
# tr0 = thead0.find_all('tr')[1] #존재하고 있는 날짜대로 findall로 모두 수집
# th0 = tr0.find_all('th')
# #날짜부분만 따로 저장
# date = []
# for i in range(len(th0)):
#     date.append(''.join(re.findall('[0-9/]',th0[i].text)))
    
# tbody0 = html22.find('tbody') #tbody에 column으로 사용할 데이터와 본문 데이터가 모두 담겨져 있다.
# tr0 = tbody0.find_all('tr')

# #columns 수집
# col = []
# for i in range(len(tr0)):
    
#     if '\xa0' in tr0[i].find('th').text:
#         tx = re.sub('\xa0','',tr0[i].find('th').text)
#     else:
#         tx = tr0[i].find('th').text
        
#     col.append(tx)
    
# #본문데아터 수집
# td = []
# for i in range(len(tr0)):
#     td0 = tr0[i].find_all('td')
#     td1 = []
#     for j in range(len(td0)):
#         if td0[j].text == '':
#             td1.append('0')
#         else:
#             td1.append(td0[j].text)
            
#     td.append(td1)
    
# td2 = list(map(list,zip(*td)))

# browser  = Chrome()
# browser.maximize_window()

# def stock_crawler(code):
#     #code = 종목번호
#     name = code
#     base_url = 'https://finance.naver.com/item/coinfo.nhn?code='+ name + '&target=finsum_more'
    
#     browser.get(base_url)
#     #frmae구조 안에 필요한 데이터가 있기 때문에 해당 데이터를 수집하기 위해서는 frame구조에 들어가야한다.
#     browser.switch_to_frame(browser.find_element_by_id('coinfo_cp'))
    
#     #재무제표 "연간" 클릭하기
#     browser.find_elements_by_xpath('//*[@class="schtab"][1]/tbody/tr/td[3]')[0].click()

#     html0 = browser.page_source
#     html1 = BeautifulSoup(html0,'html.parser')
    
#     #기업명 뽑기
#     title0 = html1.find('head').find('title').text
#     print(title0.split('-')[-1])
    
#     html22 = html1.find('table',{'class':'gHead01 all-width','summary':'주요재무정보를 제공합니다.'})
    
#     #date scrapy
#     thead0 = html22.find('thead')
#     tr0 = thead0.find_all('tr')[1]
#     th0 = tr0.find_all('th')
    
#     date = []
#     for i in range(len(th0)):
#         date.append(''.join(re.findall('[0-9/]',th0[i].text)))
    
#     #columns scrapy
#     tbody0 = html22.find('tbody')
#     tr0 = tbody0.find_all('tr')
    
#     col = []
#     for i in range(len(tr0)):

#         if '\xa0' in tr0[i].find('th').text:
#             tx = re.sub('\xa0','',tr0[i].find('th').text)
#         else:
#             tx = tr0[i].find('th').text

#         col.append(tx)
    
#     #main text scrapy
#     td = []
#     for i in range(len(tr0)):
#         td0 = tr0[i].find_all('td')
#         td1 = []
#         for j in range(len(td0)):
#             if td0[j].text == '':
#                 td1.append('0')
#             else:
#                 td1.append(td0[j].text)

#         td.append(td1)
    
#     td2 = list(map(list,zip(*td)))
    
#     return pd.DataFrame(td2,columns = col,index = date)

#====================================================================================================================================
                                        stock_craw.py backup
#====================================================================================================================================

import csv
import pandas as pd
# # from sqlalchemy import create_engine
from com_blacktensor.ext.db import db, openSession, engine
from sqlalchemy import func
# from com_blacktensor.ext.routes import Resource

# # # ============================================================
# # # ==================                     =====================
# # # ==================         KDD         =====================
# # # ==================                     =====================
# # # ============================================================
class StockKdd(object):
    keyword = input("검색어 입력: ")

    # def __init__(self):
    #     self.sk = StockKdd()
    #     self.keyword = keyword
    #     self.code_df = code_df
    #     self.code_name = code_name

    # def get_code(self, keyword, code_df):
        # print("get_code: ", keyword)
    code_df = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13',header=0)[0]
    #
    # self.code = self.code_df.query("name=='{}'".format(item_name))['code'].to_string(index=False).strip()
    # self.url = 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=code)
    #
    # 종목코드가 6자리이기 때문에 6자리를 맞춰주기 위해 설정해둠
    code_df.종목코드 = code_df.종목코드.map('{:06d}'.format)

    # 회사명과 종목코드 필요 -> 그 이외에 필요 없는 column 제외
    code_df = code_df[['회사명', '종목코드']]

    # 한글로된 컬럼명을 영어로 변환
    code_df = code_df.rename(columns={'회사명' : 'name', '종목코드' : 'code'})
    code_df.head() 
    print(code_df.head())

    # https://finance.naver.com/item/sise.nhn?code=005930(삼성전자)
    def get_url(self, keyword, code_df):
        # item_name = self.item_name
        
        # this = self.sk
        # this.code_name = code_name
        code = code_df.query("name=='{}'".format(keyword))['code'].to_string(index=False)
        code = code.strip()

        url = 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=code)
        
        print("요청 URL = {}".format(url))
        return url

    url = get_url(0, keyword, code_df)

    df = pd.DataFrame()

    for page in range(1, 16): 
        pg_url = '{url}&page={page}'.format(url=url, page=page) 
        df = df.append(pd.read_html(pg_url, header=0)[0], ignore_index=True)
        # df = df.append({'stock' : keyword}, ignore_index=True)

    df = df.dropna()

    df = df.drop(columns= {'전일비', '시가', '고가', '저가'})

    # print(df.head())
    print(df)

    df = df.rename(columns= {
        '날짜': 'date', '종가': 'close', '전일비': 'diff', '시가': 'open',
        '고가': 'high', '저가': 'low', '거래량': 'volume'
        })

    # df.drop(['diff', 'open', 'high', 'low'], axis=1, inplace=True)

    # 데이터 타입 int 변환
    df[['close', 'volume']] \
        = df[['close', 'volume']].astype(int)

    # df.drop(['diff', 'open', 'high', 'low'], axis=0, inplace=True)

    # date를 date type 변환
    df['date'] = pd.to_datetime(df['date'])

    # date 기준으로 내림차순 sort
    df = df.sort_values(by=['date'], ascending=False)

    df.loc[:, 'stock'] = keyword

    # df.head()
    print('-------------------- head -------------------')
    print(df.head())
    print('\n-------------------- 전체 -------------------')
    print(df)

    # csv file 저장
    # df.to_csv(keyword, '.csv', mode = 'a', header = False)
    df.to_csv(keyword + '_data.csv', encoding='utf8')

# ===================================================================================================================
#     # 삼성전자의 일자 데이터 url
#     # item_name=['셀트리온', '삼성전자', '하나투어']
#     # code_df = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13',header=0)[0]
#     # 한국거래소(krx)
#     item_name='셀트리온'
#     code_df = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13',header=0)[0]
#     #
#     # self.code = self.code_df.query("name=='{}'".format(item_name))['code'].to_string(index=False).strip()
#     # self.url = 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=code)
#     #
#     # 종목코드가 6자리이기 때문에 6자리를 맞춰주기 위해 설정해둠
#     code_df.종목코드 = code_df.종목코드.map('{:06d}'.format)

#     # 회사명과 종목코드 필요 -> 그 이외에 필요 없는 column 제외
#     code_df = code_df[['회사명', '종목코드']]

#     # 한글로된 컬럼명을 영어로 변환
#     code_df = code_df.rename(columns={'회사명' : 'name', '종목코드' : 'code'})
#     code_df.head() 
#     print(code_df.head())

#     # https://finance.naver.com/item/sise.nhn?code=005930(삼성전자)
#     def get_url(self, item_name, code_df):
#         # item_name = self.item_name
#         code = code_df.query("name=='{}'".format(item_name))['code'].to_string(index=False)
#         code = code.strip()

#         url = 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=code)
        
#         print("요청 URL = {}".format(url))
#         return url

#     url = get_url(0, item_name, code_df)

#     df = pd.DataFrame()

#     for page in range(1, 16): 
#         pg_url = '{url}&page={page}'.format(url=url, page=page) 
#         df = df.append(pd.read_html(pg_url, header=0)[0], ignore_index=True)

#     df = df.dropna()

#     df = df.drop(columns= {'전일비', '시가', '고가', '저가'})

#     print(df.head())

#     df = df.rename(columns= {
#         '날짜': 'date', '종가': 'close', '전일비': 'diff', '시가': 'open',
#         '고가': 'high', '저가': 'low', '거래량': 'volume'
#         })

#     # df.drop(['diff', 'open', 'high', 'low'], axis=1, inplace=True)

#     # 데이터 타입 int 변환
#     df[['close', 'volume']] \
#         = df[['close', 'volume']].astype(int)

#     # df.drop(['diff', 'open', 'high', 'low'], axis=0, inplace=True)

#     # date를 date type 변환
#     df['date'] = pd.to_datetime(df['date'])

#     # date 기준으로 내림차순 sort
#     df = df.sort_values(by=['date'], ascending=False)
    
#     # df.head()
#     print('-------------------- head -------------------')
#     print(df.head())
#     print('\n-------------------- 전체 -------------------')
#     print(df)
    
#     # csv file 저장
#     # df.to_csv('.csv', mode = 'a', header = False)

#     # # 삼성전자의 일자 데이터 url
#     # # item_name=['셀트리온', '삼성전자', '하나투어']
#     # for i in item_name:
#     #     print('\n'+i)
#     #     url = get_url(i, code_df)
#     #     # code 분리
#     #     code_url = url.split('code=')
#     #     code = 'a' + code_url[1]
#     #     print(code)

#     #     # 일자 데이터를 담을 df라는 DataFrame 정의
#     #     df = pd.DataFrame()

#     #     # 1페이지에서 15페이지의 데이터만 가져오기(약 6개월치)
#     #     for page in range(1, 16): 
#     #         pg_url = '{url}&page={page}'.format(url=url, page=page) 
#     #         df = df.append(pd.read_html(pg_url, header=0)[0], ignore_index=True)

#     #     # df.dropna()를 이용해 결측값 있는 행 제거
#     #     df = df.dropna()

#     #     # 한글 -> 영어
#     #     df = df.rename(columns= {
#     #         '날짜': 'date', '종가': 'close', '전일비': 'diff', '시가': 'open',
#     #         '고가': 'high', '저가': 'low', '거래량': 'volume'
#     #         })

#     #     # 데이터 타입 int 변환
#     #     df[['close', 'diff', 'open', 'high', 'low', 'volume']] \
#     #         = df[['close', 'diff', 'open', 'high', 'low', 'volume']].astype(int)

#     #     # date를 date type 변환
#     #     df['date'] = pd.to_datetime(df['date'])

#     #     # date 기준으로 내림차순 sort
#     #     df = df.sort_values(by=['date'], ascending=False)
        
#     #     df.head()
#     #     print('-------------------- head -------------------')
#     #     print(df.head())
#     #     print('\n-------------------- 전체 -------------------')
#     #     print(df)
        
#     #     # csv file 저장
#     #     df.to_csv(i + '.csv', mode = 'a', header = False)

# #         # Mysql Table이 존재하지 않다면 코드 이름으로 생성
# #     #     sql_table = 'SHOW TABLES LIKE \'' + code + '\''
# #     #     result = cursor.execute(sql_table)
# #     #     if result == 0:
# #     #         sql_crTable = 'CREATE TABLE ' + code + ' (Date date not null primary key, close int(11), diff int(11), open int(11), high int(11), low int(11), volume int(11));'
# #     #         cursor.execute(sql_crTable)

# #     #         # Table에 Data Insert(replace)dd
# #     #         df.to_sql(name=code, con=engine, if_exists='replace')
# #     #         conn.commit
# #     # conn.close()

# # # ============================================================
# # # ==================                     =====================
# # # ==================    Preprocessing    =====================
# # # ==================                     =====================
# # # ============================================================
# class StockDf(object):
#     ...

# ============================================================
# ==================                     =====================
# ==================       Modeling      =====================
# ==================                     =====================
# ============================================================
class StockDto(db.Model):
    __tablename__ = 'stock'
    __table_args__={'mysql_collate' : 'utf8_general_ci'}

    date : str = db.Column(db.String(10), primary_key = True, index = True)
    keyword : str = db.Column(db.String(10))
    close : int = db.Column(db.Integer)
    volume : int = db.Column(db.Integer)

    def __init__(self, date, keyword, close, volume):
        self.date = date
        self.keyword = keyword
        self.close = close
        self.volume = volume
    
    def __repr__(self):
        return f'Stock(date={self.date}, keyword={self.keyword}, close={self.close}, volume={self.volume})'

class StockVo:
    date : str = ''
    keyword : str = ''
    close : int = 0
    volume : int = 0


Session = openSession()
session = Session()
# stock_df = StockDf()


class StockDao(StockDto):
    
    @staticmethod
    def bulk():
        Session = openSession()
        session = Session()
        stock_df = StockDf()
        df = stock_df.hook()
        # print(df.head())
        session.bulk_insert_mappings(StockDto, df.to_dict(orient='records'))
        session.commit()
        session.close()

    @staticmethod
    def count():
        return session.query(func.count(StockDto.date)).one()

    @staticmethod
    def save(stock):
        new_stock = StockDto(date = stock['date'],
                           keyword = stock['keyword'],
                           close = stock['close'],
                           volume = stock['volume'])
        session.add(new_stock)
        session.commit()


# class StockTf(object):
#     ...
# class StockAi(object):
#     ...
# ============================================================
# ==================                     =====================
# ==================      Resourcing     =====================
# ==================                     =====================
# ============================================================
# # # class Stock(Resource):
# # #     ...



# # # class StockCraw():
# # #     def __init__(self, code_df, code, url):
# # #         # 삼성전자의 일자 데이터 url
# # #         self.item_name=['셀트리온', '삼성전자', '하나투어']
# # #         self.code_df = code_df
# # #         self.url = url
# # #         self.code = code

# # #     def get_code(self, code_df):
# # #         # 한국거래소(krx)
# # #         self.code_df = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13',header=0)[0]
        
# # #         # 종목코드가 6자리이기 때문에 6자리를 맞춰주기 위해 설정해둠
# # #         code_df.종목코드 = self.code_df.종목코드.map('{:06d}'.format)

# # #         # 회사명과 종목코드 필요 -> 그 이외에 필요 없는 column 제외
# # #         code_df = self.code_df[['회사명', '종목코드']]

# # #         # 한글로된 컬럼명을 영어로 변환
# # #         code_df = self.code_df.rename(columns={'회사명' : 'name', '종목코드' : 'code'})
# # #         code_df.head()
# # #         print(code_df.head())

# # #     # https://finance.naver.com/item/sise.nhn?code=005930(삼성전자)
# # #     def get_url(self, item_name, code_df, code):
# # #         self.code = code_df.query("name=='{}'".format(item_name))['code'].to_string(index=False).strip()
# # #         # code = code.strip()

# # #         url = 'http://finance.naver.com/item/sise_day.nhn?code={code}'.format(code=code)

# # #         print("요청 URL = {}".format(url))
        
# # #         # return url

# # #     def get_data(self, get_url, item_name, code_df):
# # #         for i in item_name:
# # #             print('\n'+i)
# # #             url = get_url(i, code_df)

# # #         # 일자 데이터를 담을 df라는 DataFrame 정의
# # #         self.df = pd.DataFrame()

# # #         # 1페이지에서 15페이지의 데이터만 가져오기(약 6개월치)
# # #         for page in range(1, 16): 
# # #             pg_url = '{url}&page={page}'.format(url=url, page=page) 
# # #             df = df.append(pd.read_html(pg_url, header=0)[0], ignore_index=True)

# # #         # df.dropna()를 이용해 결측값 있는 행 제거
# # #         df = df.dropna()

# # #         # df.drop()을 이용해 columns 삭제
# # #         df = df.drop(columns= {'전일비', '고가', '저가'})

# # #         # 한글 -> 영어
# # #         df = df.rename(columns= {
# # #             '날짜': 'date', '종가': 'close', '시가': 'open', '거래량': 'volume'})

# # #         # 데이터 타입 int 변환
# # #         df[['close', 'open', 'volume']] = df[['close', 'open', 'volume']].astype(int)

# # #         # date를 date type 변환
# # #         df['date'] = pd.to_datetime(df['date'])

# # #         # date 기준으로 내림차순 sort
# # #         df = df.sort_values(by=['date'], ascending=False)
        
# # #         df.head()
# # #         print('-------------------- head -------------------')
# # #         print(df.head())
# # #         print('\n-------------------- 전체 -------------------')
# # #         print(df)
        
# # #         # csv file 저장
# # #         df.to_csv(i + '.csv', mode = 'a', header = False)


# # # # class StockDto(db.Model):
# # # #     __tablename__ = 'code'
# # # #     __table_args__={'mysql_collate' : 'utf8_general_ci'}

# # # #     date : str = db.Column(db.String(10), primary_key = True, index = True)
# # # #     close : int = db.Column(db.Int)
# # # #     open : int = db.Column(db.Int)
# # # #     volume : int = db.Column(db.Integer)

# # # #     def __init__(self, date, close, open, volume):
# # # #         self.date = date
# # # #         self.close = close
# # # #         self.open = open
# # # #         self.volume = volume
    
# # # #     def __repr__(self):
# # # #         return f'Stock(date={self.date}, close={self.close}\
# # # #             , open={self.open}, volume={self.volume})'

# # # # class StockVo:
# # # #     date : str = ''
# # # #     close : int = 0
# # # #     open : int = 0
# # # #     volume : int = 0
